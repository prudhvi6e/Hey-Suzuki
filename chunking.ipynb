{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.chat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"extras/\")\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "documents = loader.load()\n",
    "data = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(data, embeddings)\n",
    "# Persist the vectors locally on disk\n",
    "vectorstore.save_local(\"vectorstore/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import streamlit as st\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from tools.llama2 import loadllama\n",
    "from tools.googlevertex import loadvertex\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "import os\n",
    "#os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    \n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        self.container = container\n",
    "        self.text=initial_text\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        # \"/\" is a marker to show difference \n",
    "        # you don't need it \n",
    "        self.text+=token+\"/\" \n",
    "        self.container.markdown(self.text) \n",
    "\n",
    "class iedge7_RAG:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.DB_FAISS_PATH = 'vectorstore/'\n",
    "\n",
    "    def conversational_chat(self,query,top_k,max_o_t,tempr):\n",
    "        \n",
    "        DB_FAISS_PATH = 'vectorstore/'\n",
    "\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "        similarity_data = db.similarity_search(query)\n",
    "\n",
    "        db_s_f = FAISS.from_texts([similarity_data[0].page_content] , embeddings)\n",
    "        \n",
    "        for i in range(top_k-1):\n",
    "            db_s_i = FAISS.from_texts([similarity_data[i+1].page_content] , embeddings)\n",
    "            db_s_f.merge_from(db_s_i)\n",
    "        \n",
    "        retriever = db_s_f.as_retriever()\n",
    "\n",
    "        db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "\n",
    "        retriever = db.as_retriever()\n",
    "\n",
    "        template = \"\"\"\n",
    "                    Use the following context about a University to answer student's question \n",
    "                    {context}\n",
    "\n",
    "                    Student Question: {question}\n",
    "\n",
    "                    Answer the question in points \n",
    "                    \n",
    "                    \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        model = ChatOpenAI(temperature = tempr, streaming=True, callbacks=[stream_handler])\n",
    "\n",
    "        chain = (\n",
    "                    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                    | prompt\n",
    "                    | model\n",
    "                    | StrOutputParser()\n",
    "                    )\n",
    "        answer = chain.invoke(query)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain-POC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
